{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Input Conll to json file\n",
    "\n",
    "This script converts the input file (already read in as dataframe) into the json input for the neural SRL scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### functions taken from part1/1_dataimport \n",
    "# copied here to have a file doing the full conversion from conll to json\n",
    "\n",
    "\n",
    "\n",
    "# retrieve longest line\n",
    "# -> required for the creation of the dataframe later\n",
    "def retrieveLength(path_to_file):\n",
    "    c = 0\n",
    "    max_line_length = -1\n",
    "    sentences = 0\n",
    "    tokens = 0\n",
    "    with open(path_to_file) as file:\n",
    "        for line in file:\n",
    "\n",
    "\n",
    "            if line.startswith('# text'):\n",
    "                sentences += 1\n",
    "            elif line.startswith('#') or line.startswith('\\n'):\n",
    "                pass\n",
    "            else:\n",
    "                values = line.split('\\t')\n",
    "                line_length = len(values)\n",
    "                if line_length > max_line_length:\n",
    "                    max_line_length = line_length\n",
    "\n",
    "                tokens += 1\n",
    "\n",
    "            c += 1   \n",
    "    \n",
    "    return max_line_length, tokens\n",
    "\n",
    "\n",
    "# conversion into dataframe\n",
    "def createDataFrame(path_to_file):\n",
    "    \n",
    "    # retrieved header according to documentation\n",
    "    conll_header = ['id', 'form', 'lemma', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc']\n",
    "\n",
    "    # header from lecture form 25.02.\n",
    "    conll_header = ['id', 'form', 'lemma', 'upos', 'xpos', 'morph', 'head', 'dep', 'head_dep', 'space', 'predicate', 'label']\n",
    "\n",
    "    # included sentenceId\n",
    "    conll_header_adapted = ['sentenceId', 'id', 'form', 'lemma', 'upos', 'xpos', 'morph', 'head', 'dep', 'head_dep', 'space', 'predicate', 'label']\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # start retrieving\n",
    "    \n",
    "    max_line_length, tokens = retrieveLength(path_to_file)\n",
    "    sentences = -1\n",
    "\n",
    "    ### create header\n",
    "    \n",
    "    # create empty dataframe with known columns and fillers for remaining collumns\n",
    "    headers_df = np.full(max_line_length + 1, np.str)  #  + 1 to add sentence column\n",
    "    \n",
    "    # add sentence column to header\n",
    "    #headers_df[1] = \n",
    "    \n",
    "    # add columns from identified columns\n",
    "    headers_df[:len(conll_header_adapted)] = conll_header_adapted\n",
    "    \n",
    "    # fill remaining column headers with '_'\n",
    "    required_length_to_fill = len(headers_df) - len(conll_header_adapted)\n",
    "    label_headers = np.full(required_length_to_fill, 'label')\n",
    "    numbers_list  = np.arange(1, required_length_to_fill + 1)\n",
    "    numbers_list = np.array([str(n) for n in numbers_list])\n",
    "    label_headers = np.char.add(label_headers, numbers_list)\n",
    "    headers_df[len(conll_header_adapted):] = label_headers\n",
    "    \n",
    "    \n",
    "    ### create dataframe\n",
    "    df = pd.DataFrame(columns=headers_df)\n",
    "    \n",
    "    ### fill dataframe\n",
    "\n",
    "    # loop through file\n",
    "    with open(path_to_file) as file:\n",
    "        for line in file:\n",
    "\n",
    "            # pass all other lines\n",
    "            if line.startswith('# text'):\n",
    "                sentences += 1\n",
    "                \n",
    "            elif line.startswith('#') or line.startswith('\\n'):\n",
    "                pass\n",
    "            \n",
    "            # only go into token lines\n",
    "            else:\n",
    "                \n",
    "                # omit linebreaks from some lines\n",
    "                if line.endswith('\\n'):\n",
    "                    line = line.replace('\\n', '')\n",
    "                \n",
    "                # split input line\n",
    "                values = np.array(line.split('\\t'))\n",
    "\n",
    "                array  = np.full(max_line_length+1, np.str)\n",
    "                \n",
    "                # add sentenceId\n",
    "                array[0] = sentences\n",
    "                # add retrieved information from conll file\n",
    "                array[1:len(values)+1] = values\n",
    "                # fill remaining columns   !!** use np.nan ?! **!! \n",
    "                array[len(values)+1:] = '_'\n",
    "    \n",
    "                # create new entry\n",
    "                df_entry = pd.DataFrame(columns=headers_df, data=[array])\n",
    "\n",
    "                # concatenate to large dataframe\n",
    "                df = pd.concat([df, df_entry], axis = 0, ignore_index=True)\n",
    "\n",
    "            #if type(sentence_limit) == int and sentences >= sentence_limit:\n",
    "            #    break\n",
    "                \n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def convertConllToJSON(path_to_file, path_to_output):\n",
    "    \n",
    "    # read dataframe in\n",
    "    df = createDataFrame(path_to_file)\n",
    "    df = df.head(100)\n",
    "\n",
    "    x = ''\n",
    "    \n",
    "    ## do conversion\n",
    "    # loop through sentences\n",
    "    for s_id in df.sentenceId.unique():\n",
    "\n",
    "        # filter for only this sentence\n",
    "        df_sentence = df[df.sentenceId == s_id].copy()   \n",
    "\n",
    "        # return indices of rows with label True of the columns of the predicates\n",
    "        indices_gold      = np.where(np.array(df_sentence.predicate) != '_' )[0]\n",
    "\n",
    "        nr_of_predicates = len(indices_gold)\n",
    "\n",
    "\n",
    "        # loop through nr_of_predicates\n",
    "        for i in range(nr_of_predicates):\n",
    "\n",
    "            # create new dict as json element\n",
    "            elem = {}\n",
    "            seq_words  = []\n",
    "            bio        = []\n",
    "            pred_sense = []\n",
    "\n",
    "\n",
    "            # create new copy for working with within this repetition of sentence\n",
    "            df_sentence_repetition = df_sentence.copy()\n",
    "            df_sentence_repetition.replace(to_replace='\"', value=';')\n",
    "            #DataFrame.replace(to_replace=None, value=NoDefault.no_default,\n",
    "        \n",
    "            # retrieve token forms\n",
    "            seq_words  = list(df_sentence_repetition.form)\n",
    "\n",
    "            # assign pred_sense\n",
    "            pred_sense.append(int(indices_gold[i]))\n",
    "            pred_sense.append(np.array(df_sentence_repetition.predicate)[indices_gold[i]])\n",
    "            pred_sense.append('_')\n",
    "            pred_sense.append(np.array(df_sentence_repetition.xpos)[indices_gold[i]])\n",
    "\n",
    "\n",
    "            ## labels\n",
    "\n",
    "            # -> transform labels from all label columns to this one column\n",
    "\n",
    "            # create filler array\n",
    "            label_array = np.full(len(df_sentence_repetition), '0')\n",
    "\n",
    "            # slice df_sentence\n",
    "            row = df_sentence.iloc[indices_gold[i], :]\n",
    "            list_of_column_indices_with_V = np.where(np.array(row) == 'V')[0]\n",
    "\n",
    "            # sanity check -> columns found with V should be 1\n",
    "            if len(list_of_column_indices_with_V) == 1:\n",
    "\n",
    "                # do conversion\n",
    "\n",
    "                # find respective_label_column\n",
    "                respective_column_index = list_of_column_indices_with_V[0]\n",
    "\n",
    "                # retrieve column\n",
    "                respective_label_column = np.array(df_sentence.iloc[:, respective_column_index])\n",
    "\n",
    "                # replave '_' label with '0'\n",
    "                respective_label_column[respective_label_column == '_'] = '0'\n",
    "\n",
    "                # overwrite filler with retrieved labels\n",
    "                label_array = respective_label_column\n",
    "\n",
    "            # label_array remains only filled with '_' because no (coherent) labels could be found\n",
    "            else:\n",
    "                pass\n",
    "            # assign retrieved array\n",
    "            #df_sentence_repetition['label_gold']        = label_array\n",
    "            for i in range(len(label_array)):\n",
    "                if label_array[i] != '0':\n",
    "                    label_array[i] = 'B-' + label_array[i]\n",
    "\n",
    "\n",
    "            bio = list(label_array)\n",
    "            elem[\"seq_words\"]  = seq_words\n",
    "            elem[\"BIO\"]        = bio\n",
    "            elem[\"pred_sense\"] = pred_sense\n",
    "\n",
    "            x += json.dumps(elem) + '\\n'\n",
    "            \n",
    "    with open(path_to_output, 'a') as outfile:\n",
    "        outfile.write(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zf/wykfcg015tggw4hgf1dfjbz00000gn/T/ipykernel_75144/2816220653.py:57: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  headers_df = np.full(max_line_length + 1, np.str)  #  + 1 to add sentence column\n",
      "/var/folders/zf/wykfcg015tggw4hgf1dfjbz00000gn/T/ipykernel_75144/2816220653.py:101: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  array  = np.full(max_line_length+1, np.str)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_to_file = '../data/input/srl_univprop_en.dev_excerpt.conll' #'../data/input/srl_univprop_en.train_excerpt.conll' for training set\n",
    "path_to_output = '../data/intermediate/neuralSRL_dev_excerpt.jsonl'  #'../data/intermediate/neuralSRL_train_excerpt.jsonl' for training set\n",
    "convertConllToJSON(path_to_file, path_to_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
